{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import analysis_functions as af\n",
    "import analysis_functions2 as af2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = \"tac.pkl\"\n",
    "sd = af.get_pickle(sd_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### print score ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "af.print_score_ranges(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### how many summaries per document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean([len(d['system_summaries']) for d in sd.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### what metrics found for each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlist = af.get_metrics_list(sd)\n",
    "mlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist = ['bert_recall_score', 'mover_score', 'rouge_1_recall', 'rouge_2_recall', 'rouge_l_recall', 'js-2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ease of summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_type = 'avg__all_max_m'\n",
    "y_type = 'ktau'\n",
    "metrics = mlist\n",
    "for x_modifier in ['doc']:\n",
    "    af2.plot_doc(sd, x_type, y_type, x_modifier=x_modifier, abs_pr_plot_cutoff=0, \n",
    "                 fit_line_window=10, metrics_list=mlist, show_title=True, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_type = 'ref_abstractiveness_wrt_doc'\n",
    "y_type = 'ktau'\n",
    "metrics = mlist\n",
    "for x_modifier in ['doc']:\n",
    "    af2.plot_doc(sd, x_type, y_type, x_modifier=x_modifier, abs_pr_plot_cutoff=0, \n",
    "                 fit_line_window=15, metrics_list=mlist, show_y_label=False, show_yticklabels=True, \n",
    "                 show_title=True, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_type = 'coverage'\n",
    "y_type = 'ktau'\n",
    "                \n",
    "af2.plot_doc(sd, x_type, y_type, x_modifier=None, \n",
    "             abs_pr_plot_cutoff=0, fit_line_window=10,\n",
    "             cutoff_metric=None, percentile=None, metrics_list=mlist, show_y_label=False, show_yticklabels=True, \n",
    "            show_title=True, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disagreement_utils import get_pairwise_disagreement\n",
    "from disagreement_utils import non_cum_get_pairwise_disagreement\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = {'bert_recall_score': 'BScore', 'mover_score': 'MS', \n",
    "              'rouge_1_recall': 'R1', 'rouge_2_recall': 'R2', 'rouge_l_recall': 'RL', 'js-2': 'JS2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for doc_id, summaries in sd.items():\n",
    "    data[doc_id] = [{\n",
    "        'text': summ['system_summary'],\n",
    "        **summ['scores']\n",
    "    } for summ in summaries['system_summaries'].values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for pair in itertools.combinations(mlist, 2):\n",
    "    mat = get_pairwise_disagreement(pair[0], pair[1], [data])\n",
    "    metric_string = f'({short_name[pair[0]]}, {short_name[pair[1]]})'\n",
    "    df_random = pd.DataFrame(mat, columns=['average pair score', metric_string])\n",
    "    if ax == None:\n",
    "        ax = df_random.plot(x='average pair score', y=metric_string, figsize=(10, 7), linewidth=3)\n",
    "    else:\n",
    "        ax = df_random.plot(x='average pair score', y=metric_string, ax=ax, linewidth=3)\n",
    "        \n",
    "ax.legend(prop={'size': 16}, loc='upper left')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Disagreement (%)', fontsize=35)\n",
    "ax.tick_params(axis='both', labelsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = None\n",
    "for pair in itertools.combinations(mlist, 2):\n",
    "    mat = non_cum_get_pairwise_disagreement(pair[0], pair[1], [data])\n",
    "    metric_string = f'({short_name[pair[0]]}, {short_name[pair[1]]})'\n",
    "    df_random = pd.DataFrame(mat, columns=['average pair score', metric_string])\n",
    "    if ax == None:\n",
    "        ax = df_random.plot(x='average pair score', y=metric_string, figsize=(10, 7), linewidth=3)\n",
    "    else:\n",
    "        ax = df_random.plot(x='average pair score', y=metric_string, ax=ax, linewidth=3)\n",
    "        \n",
    "# ax.legend(prop={'size': 16})\n",
    "ax.set_xlabel('')\n",
    "# ax.set_ylabel('Disagreement (%)', fontsize=25)\n",
    "ax.tick_params(axis='both', labelsize=25)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of overall improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disagreement_utils import proportion_better\n",
    "from disagreement_utils import proportion_worse\n",
    "from plotting_utils import plot_proportion_improvements\n",
    "from plotting_utils import plot_proportion_worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = proportion_better([data], m=1000)\n",
    "datapoints.extend(proportion_better([data], m=1000, min_avg=0.2))\n",
    "datapoints.extend(proportion_better([data], m=1500, min_avg=0.3))\n",
    "datapoints.extend(proportion_better([data], m=2500, min_avg=0.35))\n",
    "plot_proportion_improvements(datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of overall improvements with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_scores(mlist):\n",
    "    return {m: random.random() for m in mlist}\n",
    "\n",
    "data_r = {}\n",
    "for doc_id, summs in data.items():\n",
    "    data_r[doc_id] = [{\n",
    "        'text': summ['text'],\n",
    "        **get_random_scores(mlist)\n",
    "    } for summ in summs]\n",
    "    \n",
    "datapoints = proportion_better([data_r], m=1000)\n",
    "datapoints.extend(proportion_better([data_r], m=1000, min_avg=0.2))\n",
    "datapoints.extend(proportion_better([data_r], m=1500, min_avg=0.3))\n",
    "datapoints.extend(proportion_better([data_r], m=2500, min_avg=0.35))\n",
    "plot_proportion_improvements(datapoints, show_ylabel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = proportion_worse([data], m=1000)\n",
    "datapoints.extend(proportion_worse([data], m=1000, min_avg=0.2))\n",
    "datapoints.extend(proportion_worse([data], m=1500, min_avg=0.3))\n",
    "datapoints.extend(proportion_worse([data], m=2500, min_avg=0.35))\n",
    "plot_proportion_worse(datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion worse with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = proportion_worse([data_r], m=1000)\n",
    "datapoints.extend(proportion_worse([data_r], m=1000, min_avg=0.2))\n",
    "datapoints.extend(proportion_worse([data_r], m=1500, min_avg=0.3))\n",
    "datapoints.extend(proportion_worse([data_r], m=2500, min_avg=0.35))\n",
    "plot_proportion_worse(datapoints, show_ylabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
